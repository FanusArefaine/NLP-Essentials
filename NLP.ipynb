{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"ham\\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\nham\\tNah I don't think he goes to usf, he lives around here though\\nham\\tEven my brother is not like to speak with me. They treat me like aid\""
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "# Reading text data \n",
    "raw_data = open('SMSSpamCollection.tsv', encoding='utf-8').read()\n",
    "\n",
    "# printing the first 500 characters of the text data\n",
    "raw_data[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ham',\n \"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\",\n 'spam',\n \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n 'ham']"
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "# Parsing the text data\n",
    "parsed_data = raw_data.replace('\\t', '\\n').split('\\n')\n",
    "\n",
    "#Printing the parse data\n",
    "parsed_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ham', 'spam', 'ham', 'ham', 'ham']"
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "# Extracting the labels from the text data\n",
    "\n",
    "labels = parsed_data[0::2]\n",
    "labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[\"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\",\n \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n \"Nah I don't think he goes to usf, he lives around here though\",\n 'Even my brother is not like to speak with me. They treat me like aids patent.',\n 'I HAVE A DATE ON SUNDAY WITH WILL!!']"
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "# Extracting the text data \n",
    "text_list = parsed_data[1::2]\n",
    "text_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5571\n5570\n"
    }
   ],
   "source": [
    "# Labels seem to have one extra element\n",
    "print(len(labels))\n",
    "print(len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ham', 'ham', 'ham', 'ham', '']"
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "source": [
    "# Checking labels for one extra element\n",
    "\n",
    "# We can see that the label list has one empty element at the end of the list\n",
    "labels[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['spam', 'ham', 'ham', 'ham', 'ham']"
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "source": [
    "# Dropping the last element from label list\n",
    "labels.pop()\n",
    "\n",
    "# Checking\n",
    "labels[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the data to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  label  \\\n0   ham   \n1  spam   \n2   ham   \n3   ham   \n4   ham   \n\n                                                                                             body_list  \n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n2                                        Nah I don't think he goes to usf, he lives around here though  \n3                        Even my brother is not like to speak with me. They treat me like aids patent.  \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>body_list</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "# Converting the list of text data and labels to a single dataframe\n",
    "\n",
    "full_corpus = pd.DataFrame({\n",
    "    'label': labels,\n",
    "    'body_list': text_list\n",
    "})\n",
    "\n",
    "full_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data easily using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pd_corpus = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header=None)\n",
    "full_pd_corpus.columns = ['labels', 'text_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  labels  \\\n0    ham   \n1   spam   \n2    ham   \n3    ham   \n4    ham   \n\n                                                                                             text_data  \n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n2                                        Nah I don't think he goes to usf, he lives around here though  \n3                        Even my brother is not like to speak with me. They treat me like aids patent.  \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "source": [
    "full_pd_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the data for later use\n",
    "body_text = full_pd_corpus.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(5568, 2)"
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "# Shape of the data\n",
    "\n",
    "# (rows, columns)\n",
    "full_pd_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ham     4822\nspam     746\nName: labels, dtype: int64"
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "# Proportions of spam and ham \n",
    "full_pd_corpus['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "labels       0\ntext_data    0\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "# Check if there are any missing values \n",
    "\n",
    "# As shown here, there are no missing values\n",
    "full_pd_corpus.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_test = 'This is a made up string to test 2 different regex methods'\n",
    "re_test_messy = 'This      is a made up     string to test 2    different regex methods'\n",
    "re_test_messy1 = 'This-is-a-made/up.string*to>>>>test----2\"\"\"\"\"\"different~regex-methods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This',\n 'is',\n 'a',\n 'made',\n 'up',\n 'string',\n 'to',\n 'test',\n '2',\n 'different',\n 'regex',\n 'methods']"
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "source": [
    "# Splitting a sentence into a list of words\n",
    "# '/s' . - looking for a single whitespaces\n",
    "\n",
    "re.split('\\s', re_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This',\n '',\n '',\n '',\n '',\n '',\n 'is',\n 'a',\n 'made',\n 'up',\n '',\n '',\n '',\n '',\n 'string',\n 'to',\n 'test',\n '2',\n '',\n '',\n '',\n 'different',\n 'regex',\n 'methods']"
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "re.split('\\s', re_test_messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This',\n 'is',\n 'a',\n 'made',\n 'up',\n 'string',\n 'to',\n 'test',\n '2',\n 'different',\n 'regex',\n 'methods']"
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "# '/s+' . - Looking for more than single white space\n",
    "re.split('\\s+', re_test_messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This-is-a-made/up.string*to>>>>test----2\"\"\"\"\"\"different~regex-methods']"
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": [
    "re.split('\\s', re_test_messy1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This',\n 'is',\n 'a',\n 'made',\n 'up',\n 'string',\n 'to',\n 'test',\n '2',\n 'different',\n 'regex',\n 'methods']"
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "# '\\W+' - search for a non word charachter to define a split point\n",
    "re.split('\\W+', re_test_messy1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This',\n 'is',\n 'a',\n 'made',\n 'up',\n 'string',\n 'to',\n 'test',\n '2',\n 'different',\n 'regex',\n 'methods']"
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "# Search for the words instead of the splitting elements\n",
    "# \\S+  - search for one or more non white space characters\n",
    "re.findall('\\S+', re_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This',\n 'is',\n 'a',\n 'made',\n 'up',\n 'string',\n 'to',\n 'test',\n '2',\n 'different',\n 'regex',\n 'methods']"
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "re.findall('\\S+', re_test_messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This-is-a-made/up.string*to>>>>test----2\"\"\"\"\"\"different~regex-methods']"
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "# Search for one of more non white space characters\n",
    "re.findall('\\S+', re_test_messy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This',\n 'is',\n 'a',\n 'made',\n 'up',\n 'string',\n 'to',\n 'test',\n '2',\n 'different',\n 'regex',\n 'methods']"
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "# \\w+ .  - looks for one or more word characters\n",
    "re.findall('\\w+', re_test_messy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing a specific string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "pep8_test = 'I try to follow PEP8 guidelines'\n",
    "pep7_test = 'I try to follow PEP7 guidelines'\n",
    "peep8_test = 'I try to follow PEEP8 guidelines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['try', 'to', 'follow', 'guidelines']"
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "# Search only small letters\n",
    "re.findall('[a-z]+', pep8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['I', 'PEP']"
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "# Search only capital letters\n",
    "re.findall('[A-Z]+', pep8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['PEP8']"
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "# Search capital letters with numbers\n",
    "re.findall('[A-Z]+[0-9]+', pep8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['PEP7']"
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "# Search capital letters with numbers\n",
    "re.findall('[A-Z]+[0-9]+', pep7_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['PEEP8']"
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "# Search capital letters with numbers\n",
    "re.findall('[A-Z]+[0-9]+', peep8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'I try to follow PEP8 Python Styleguide guidelines'"
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "# substuting text data\n",
    "re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide', pep8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'I try to follow PEP8 Python Styleguide guidelines'"
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "# substuting text data\n",
    "re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide', pep7_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'I try to follow PEP8 Python Styleguide guidelines'"
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "# substuting text data\n",
    "re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide', peep8_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  labels  \\\n0    ham   \n1   spam   \n2    ham   \n3    ham   \n4    ham   \n\n                                                                                             text_data  \n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n2                                        Nah I don't think he goes to usf, he lives around here though  \n3                        Even my brother is not like to speak with me. They treat me like aids patent.  \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "full_pd_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  labels  \\\n0    ham   \n1   spam   \n2    ham   \n3    ham   \n4    ham   \n\n                                                                                             text_data  \\\n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n2                                        Nah I don't think he goes to usf, he lives around here though   \n3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n\n                                                                                         no_punct_text  \n0  Ive been searching for the right words to thank you for this breather I promise i wont take your...  \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...  \n2                                          Nah I dont think he goes to usf he lives around here though  \n3                          Even my brother is not like to speak with me They treat me like aids patent  \n4                                                                    I HAVE A DATE ON SUNDAY WITH WILL  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text_data</th>\n      <th>no_punct_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>Nah I dont think he goes to usf he lives around here though</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "full_pd_corpus['no_punct_text'] = full_pd_corpus['text_data'].apply(lambda x: remove_punct(x))\n",
    "full_pd_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to tokenize the text data\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  labels  \\\n0    ham   \n1   spam   \n2    ham   \n3    ham   \n4    ham   \n\n                                                                                             text_data  \\\n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n2                                        Nah I don't think he goes to usf, he lives around here though   \n3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n\n                                                                                         no_punct_text  \\\n0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n2                                          Nah I dont think he goes to usf he lives around here though   \n3                          Even my brother is not like to speak with me They treat me like aids patent   \n4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n\n                                                                                   body_text_tokenized  \n0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...  \n1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...  \n2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  \n3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]  \n4                                                           [i, have, a, date, on, sunday, with, will]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text_data</th>\n      <th>no_punct_text</th>\n      <th>body_text_tokenized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>Nah I dont think he goes to usf he lives around here though</td>\n      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n      <td>[i, have, a, date, on, sunday, with, will]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "full_pd_corpus['body_text_tokenized'] = full_pd_corpus['no_punct_text'].apply(lambda x: tokenize(x.lower()))\n",
    "full_pd_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "stopword[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    no_stop_words = [word for word in text if word not in stopword]\n",
    "    return no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  labels  \\\n0    ham   \n1   spam   \n2    ham   \n3    ham   \n4    ham   \n\n                                                                                             text_data  \\\n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n2                                        Nah I don't think he goes to usf, he lives around here though   \n3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n\n                                                                                         no_punct_text  \\\n0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n2                                          Nah I dont think he goes to usf he lives around here though   \n3                          Even my brother is not like to speak with me They treat me like aids patent   \n4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n\n                                                                                   body_text_tokenized  \\\n0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...   \n1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n4                                                           [i, have, a, date, on, sunday, with, will]   \n\n                                                                                      text_nostopwords  \n0  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...  \n1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n2                                                 [nah, dont, think, goes, usf, lives, around, though]  \n3                                              [even, brother, like, speak, treat, like, aids, patent]  \n4                                                                                       [date, sunday]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text_data</th>\n      <th>no_punct_text</th>\n      <th>body_text_tokenized</th>\n      <th>text_nostopwords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>Nah I dont think he goes to usf he lives around here though</td>\n      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n      <td>[i, have, a, date, on, sunday, with, will]</td>\n      <td>[date, sunday]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "full_pd_corpus['text_nostopwords'] = full_pd_corpus['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "full_pd_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['MARTIN_EXTENSIONS',\n 'NLTK_EXTENSIONS',\n 'ORIGINAL_ALGORITHM',\n '__abstractmethods__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__unicode__',\n '__weakref__',\n '_abc_cache',\n '_abc_negative_cache',\n '_abc_negative_cache_version',\n '_abc_registry',\n '_apply_rule_list',\n '_contains_vowel',\n '_ends_cvc',\n '_ends_double_consonant',\n '_has_positive_measure',\n '_is_consonant',\n '_measure',\n '_replace_suffix',\n '_step1a',\n '_step1b',\n '_step1c',\n '_step2',\n '_step3',\n '_step4',\n '_step5a',\n '_step5b',\n 'mode',\n 'pool',\n 'stem',\n 'unicode_repr',\n 'vowels']"
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "dir(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "grow\ngrow\ngrow\n"
    }
   ],
   "source": [
    "# Checking porter stemmer\n",
    "print(ps.stem('grows'))\n",
    "print(ps.stem('growing'))\n",
    "print(ps.stem('grow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "run\nrun\nrunner\n"
    }
   ],
   "source": [
    "# Checking porter stemmer\n",
    "print(ps.stem('run'))\n",
    "print(ps.stem('running'))\n",
    "print(ps.stem('runner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  label  \\\n0   ham   \n1  spam   \n2   ham   \n3   ham   \n4   ham   \n\n                                                                                             body_list  \n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n2                                        Nah I don't think he goes to usf, he lives around here though  \n3                        Even my brother is not like to speak with me. They treat me like aids patent.  \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>body_list</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "full_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A comprehensive function to clean the text\n",
    "def clean_text(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopword]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  label  \\\n0   ham   \n1  spam   \n2   ham   \n3   ham   \n4   ham   \n\n                                                                                             body_list  \\\n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n2                                        Nah I don't think he goes to usf, he lives around here though   \n3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n\n                                                                                       clean_body_text  \n0  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...  \n1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n2                                                 [nah, dont, think, goes, usf, lives, around, though]  \n3                                              [even, brother, like, speak, treat, like, aids, patent]  \n4                                                                                       [date, sunday]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>body_list</th>\n      <th>clean_body_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>[date, sunday]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "full_corpus['clean_body_text'] = full_corpus['body_list'].apply(lambda x: clean_text(x.lower()))\n",
    "full_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steming function\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  label  \\\n0   ham   \n1  spam   \n2   ham   \n3   ham   \n4   ham   \n\n                                                                                             body_list  \\\n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n2                                        Nah I don't think he goes to usf, he lives around here though   \n3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n\n                                                                                       clean_body_text  \\\n0  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...   \n1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n2                                                 [nah, dont, think, goes, usf, lives, around, though]   \n3                                              [even, brother, like, speak, treat, like, aids, patent]   \n4                                                                                       [date, sunday]   \n\n                                                                                     body_text_stemmed  \n0  [ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, won...  \n1  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...  \n2                                                   [nah, dont, think, goe, usf, live, around, though]  \n3                                               [even, brother, like, speak, treat, like, aid, patent]  \n4                                                                                       [date, sunday]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>body_list</th>\n      <th>clean_body_text</th>\n      <th>body_text_stemmed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n      <td>[ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, won...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>[date, sunday]</td>\n      <td>[date, sunday]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 173
    }
   ],
   "source": [
    "full_corpus['body_text_stemmed'] = full_corpus['clean_body_text'].apply(lambda x: stemming(x))\n",
    "\n",
    "full_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordnet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a lemmatizer\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__unicode__',\n '__weakref__',\n 'lemmatize',\n 'unicode_repr']"
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "dir(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "mean\nmean\n"
    }
   ],
   "source": [
    "# checking more stemming \n",
    "print(ps.stem('meaning'))\n",
    "print(ps.stem('meanness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "meanness\nmeaning\n"
    }
   ],
   "source": [
    "# Checking lemmatizer\n",
    "print(wn.lemmatize('meanness'))\n",
    "print(wn.lemmatize('meaning'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "goos\ngees\n"
    }
   ],
   "source": [
    "# checking more stemming \n",
    "print(ps.stem('goose'))\n",
    "print(ps.stem('geese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "goose\ngoose\n"
    }
   ],
   "source": [
    "# Checking lemmatizer\n",
    "print(wn.lemmatize('goose'))\n",
    "print(wn.lemmatize('geese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying lemmatizer to the cleaned text body\n",
    "def lemmaztize(text):\n",
    "    lem = [wn.lemmatize(word) for word in text]\n",
    "    return lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  label  \\\n0   ham   \n1  spam   \n2   ham   \n3   ham   \n4   ham   \n\n                                                                                             body_list  \\\n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n2                                        Nah I don't think he goes to usf, he lives around here though   \n3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n\n                                                                                       clean_body_text  \\\n0  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...   \n1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n2                                                 [nah, dont, think, goes, usf, lives, around, though]   \n3                                              [even, brother, like, speak, treat, like, aids, patent]   \n4                                                                                       [date, sunday]   \n\n                                                                                     body_text_stemmed  \\\n0  [ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, won...   \n1  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...   \n2                                                   [nah, dont, think, goe, usf, live, around, though]   \n3                                               [even, brother, like, speak, treat, like, aid, patent]   \n4                                                                                       [date, sunday]   \n\n                                                                                   lemmatized_textbody  \n0  [ive, searching, right, word, thank, breather, promise, wont, take, help, granted, fulfil, promi...  \n1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n2                                                    [nah, dont, think, go, usf, life, around, though]  \n3                                               [even, brother, like, speak, treat, like, aid, patent]  \n4                                                                                       [date, sunday]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>body_list</th>\n      <th>clean_body_text</th>\n      <th>body_text_stemmed</th>\n      <th>lemmatized_textbody</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n      <td>[ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, won...</td>\n      <td>[ive, searching, right, word, thank, breather, promise, wont, take, help, granted, fulfil, promi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n      <td>[nah, dont, think, go, usf, life, around, though]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>[date, sunday]</td>\n      <td>[date, sunday]</td>\n      <td>[date, sunday]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "source": [
    "full_corpus['lemmatized_textbody'] = full_corpus['clean_body_text'].apply(lambda x: lemmaztize(x))\n",
    "\n",
    "full_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to clean the dataset\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopword]\n",
    "    stemmed = [ps.stem(word) for word in text]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  labels  \\\n0    ham   \n1   spam   \n2    ham   \n3    ham   \n4    ham   \n\n                                                                                             text_data  \n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n2                                        Nah I don't think he goes to usf, he lives around here though  \n3                        Even my brother is not like to speak with me. They treat me like aids patent.  \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 202
    }
   ],
   "source": [
    "body_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5568, 8337)\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_counts = count_vect.fit_transform(body_text['text_data'])\n",
    "\n",
    "# Shape of the vectorized data\n",
    "print(X_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['', '0', '008704050406', '0089mi', '0121', '01223585236', '01223585334', '0125698789', '02', '020603', '0207', '02070836089', '02072069400', '02073162414', '02085076972', '020903', '021', '050703', '0578', '06', '060505', '061104', '07008009200', '07046744435', '07090201529', '07090298926', '07099833605', '071104', '07123456789', '0721072', '07732584351', '07734396839', '07742676969', '07753741225', '0776xxxxxxx', '07786200117', '077xxx', '078', '07801543489', '07808', '07808247860', '07808726822', '07815296484', '07821230901', '0784987', '0789xxxxxxx', '0794674629107880867867', '0796xxxxxx', '07973788240', '07xxxxxxxxx']\n"
    }
   ],
   "source": [
    "# Printing all the features\n",
    "print(count_vect.get_feature_names()[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<5568x8337 sparse matrix of type '<class 'numpy.int64'>'\n\twith 55740 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 212
    }
   ],
   "source": [
    "# Count vectorizer output is a sparse matrix\n",
    "X_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   0     1     2     3     4     5     6     7     8     9     ...  8327  \\\n0     0     0     0     0     0     0     0     0     0     0  ...     0   \n1     0     0     0     0     0     0     0     0     0     0  ...     0   \n2     0     0     0     0     0     0     0     0     0     0  ...     0   \n3     0     0     0     0     0     0     0     0     0     0  ...     0   \n4     0     0     0     0     0     0     0     0     0     0  ...     0   \n\n   8328  8329  8330  8331  8332  8333  8334  8335  8336  \n0     0     0     0     0     0     0     0     0     0  \n1     0     0     0     0     0     0     0     0     0  \n2     0     0     0     0     0     0     0     0     0  \n3     0     0     0     0     0     0     0     0     0  \n4     0     0     0     0     0     0     0     0     0  \n\n[5 rows x 8337 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>8327</th>\n      <th>8328</th>\n      <th>8329</th>\n      <th>8330</th>\n      <th>8331</th>\n      <th>8332</th>\n      <th>8333</th>\n      <th>8334</th>\n      <th>8335</th>\n      <th>8336</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 8337 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 214
    }
   ],
   "source": [
    "# Changing a sparse matrix to a dataframe\n",
    "\n",
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      0  008704050406  0089mi  0121  01223585236  01223585334  0125698789  02  \\\n0  0  0             0       0     0            0            0           0   0   \n1  0  0             0       0     0            0            0           0   0   \n2  0  0             0       0     0            0            0           0   0   \n3  0  0             0       0     0            0            0           0   0   \n4  0  0             0       0     0            0            0           0   0   \n\n   020603  ...  zoe  zogtoriu  zoom  zouk  zyada  Ü  é  ü  üll  〨ud  \n0       0  ...    0         0     0     0      0  0  0  0    0    0  \n1       0  ...    0         0     0     0      0  0  0  0    0    0  \n2       0  ...    0         0     0     0      0  0  0  0    0    0  \n3       0  ...    0         0     0     0      0  0  0  0    0    0  \n4       0  ...    0         0     0     0      0  0  0  0    0    0  \n\n[5 rows x 8337 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>0</th>\n      <th>008704050406</th>\n      <th>0089mi</th>\n      <th>0121</th>\n      <th>01223585236</th>\n      <th>01223585334</th>\n      <th>0125698789</th>\n      <th>02</th>\n      <th>020603</th>\n      <th>...</th>\n      <th>zoe</th>\n      <th>zogtoriu</th>\n      <th>zoom</th>\n      <th>zouk</th>\n      <th>zyada</th>\n      <th>Ü</th>\n      <th>é</th>\n      <th>ü</th>\n      <th>üll</th>\n      <th>〨ud</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 8337 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 217
    }
   ],
   "source": [
    "# Fixing the column names into the dataframe\n",
    "\n",
    "X_counts_df.columns = count_vect.get_feature_names()\n",
    "X_counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning text data for N-Grams\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join(ps.stem(word) for word in tokens if word not in stopword)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = body_text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  labels  \\\n0    ham   \n1   spam   \n2    ham   \n3    ham   \n4    ham   \n\n                                                                                             text_data  \\\n0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n2                                        Nah I don't think he goes to usf, he lives around here though   \n3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n\n                                                                                          cleaned_text  \n0  ive search right word thank breather I promis wont take help grant fulfil promis you wonder bles...  \n1  free entri 2 wkli comp win FA cup final tkt 21st may 2005 text FA 87121 receiv entri questionstd...  \n2                                                          nah I dont think goe usf live around though  \n3                                                   even brother like speak they treat like aid patent  \n4                                                                    I have A date ON sunday with will  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text_data</th>\n      <th>cleaned_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n      <td>ive search right word thank breather I promis wont take help grant fulfil promis you wonder bles...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>free entri 2 wkli comp win FA cup final tkt 21st may 2005 text FA 87121 receiv entri questionstd...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>nah I dont think goe usf live around though</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>even brother like speak they treat like aid patent</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>I have A date ON sunday with will</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 222
    }
   ],
   "source": [
    "text_corpus['cleaned_text'] = text_corpus['text_data'].apply(lambda x: clean_text(x))\n",
    "text_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply N-Gram Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5568, 34129)\n['008704050406 sp', '0089mi last', '0121 2025050', '01223585236 xx', '01223585334 cum', '0125698789 ring', '02 user', '020603 2nd', '020603 thi', '0207 153']\n"
    }
   ],
   "source": [
    "ngram_vect = CountVectorizer(ngram_range=(2,2))\n",
    "X_counts = ngram_vect.fit_transform(text_corpus['cleaned_text'])\n",
    "\n",
    "print(X_counts.shape)\n",
    "print(ngram_vect.get_feature_names()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<5568x34129 sparse matrix of type '<class 'numpy.int64'>'\n\twith 48076 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 227
    }
   ],
   "source": [
    "# The output is sparse matrix\n",
    "X_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   008704050406 sp  0089mi last  0121 2025050  01223585236 xx  \\\n0                0            0             0               0   \n1                0            0             0               0   \n2                0            0             0               0   \n3                0            0             0               0   \n4                0            0             0               0   \n\n   01223585334 cum  0125698789 ring  02 user  020603 2nd  020603 thi  \\\n0                0                0        0           0           0   \n1                0                0        0           0           0   \n2                0                0        0           0           0   \n3                0                0        0           0           0   \n4                0                0        0           0           0   \n\n   0207 153  ...  zoe 18  zoe it  zogtoriu stare  zoom cine  zouk with  \\\n0         0  ...       0       0               0          0          0   \n1         0  ...       0       0               0          0          0   \n2         0  ...       0       0               0          0          0   \n3         0  ...       0       0               0          0          0   \n4         0  ...       0       0               0          0          0   \n\n   zyada kisi  üll finish  üll submit  üll take  〨ud even  \n0           0           0           0         0         0  \n1           0           0           0         0         0  \n2           0           0           0         0         0  \n3           0           0           0         0         0  \n4           0           0           0         0         0  \n\n[5 rows x 34129 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>008704050406 sp</th>\n      <th>0089mi last</th>\n      <th>0121 2025050</th>\n      <th>01223585236 xx</th>\n      <th>01223585334 cum</th>\n      <th>0125698789 ring</th>\n      <th>02 user</th>\n      <th>020603 2nd</th>\n      <th>020603 thi</th>\n      <th>0207 153</th>\n      <th>...</th>\n      <th>zoe 18</th>\n      <th>zoe it</th>\n      <th>zogtoriu stare</th>\n      <th>zoom cine</th>\n      <th>zouk with</th>\n      <th>zyada kisi</th>\n      <th>üll finish</th>\n      <th>üll submit</th>\n      <th>üll take</th>\n      <th>〨ud even</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 34129 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "source": [
    "# transforming a sparse matrix into dataframe\n",
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df.columns = ngram_vect.get_feature_names()\n",
    "X_counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to clean the dataset for TF-IDF\n",
    "def clean_text_data(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5568, 8337)\n['', '0', '008704050406', '0089mi', '0121', '01223585236', '01223585334', '0125698789', '02', '020603', '0207', '02070836089', '02072069400', '02073162414', '02085076972', '020903', '021', '050703', '0578', '06']\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text_data)\n",
    "X_tfidf = tfidf_vect.fit_transform(body_text['text_data'])\n",
    "print(X_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names()[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<5568x8337 sparse matrix of type '<class 'numpy.float64'>'\n\twith 55740 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 244
    }
   ],
   "source": [
    "# output of the vectorizer is a sparse matrix\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   008704050406 sp  0089mi last  0121 2025050  01223585236 xx  \\\n0                0            0             0               0   \n1                0            0             0               0   \n2                0            0             0               0   \n3                0            0             0               0   \n4                0            0             0               0   \n\n   01223585334 cum  0125698789 ring  02 user  020603 2nd  020603 thi  \\\n0                0                0        0           0           0   \n1                0                0        0           0           0   \n2                0                0        0           0           0   \n3                0                0        0           0           0   \n4                0                0        0           0           0   \n\n   0207 153  ...  zoe 18  zoe it  zogtoriu stare  zoom cine  zouk with  \\\n0         0  ...       0       0               0          0          0   \n1         0  ...       0       0               0          0          0   \n2         0  ...       0       0               0          0          0   \n3         0  ...       0       0               0          0          0   \n4         0  ...       0       0               0          0          0   \n\n   zyada kisi  üll finish  üll submit  üll take  〨ud even  \n0           0           0           0         0         0  \n1           0           0           0         0         0  \n2           0           0           0         0         0  \n3           0           0           0         0         0  \n4           0           0           0         0         0  \n\n[5 rows x 34129 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>008704050406 sp</th>\n      <th>0089mi last</th>\n      <th>0121 2025050</th>\n      <th>01223585236 xx</th>\n      <th>01223585334 cum</th>\n      <th>0125698789 ring</th>\n      <th>02 user</th>\n      <th>020603 2nd</th>\n      <th>020603 thi</th>\n      <th>0207 153</th>\n      <th>...</th>\n      <th>zoe 18</th>\n      <th>zoe it</th>\n      <th>zogtoriu stare</th>\n      <th>zoom cine</th>\n      <th>zouk with</th>\n      <th>zyada kisi</th>\n      <th>üll finish</th>\n      <th>üll submit</th>\n      <th>üll take</th>\n      <th>〨ud even</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 34129 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 245
    }
   ],
   "source": [
    "# transfroming a sparse matrix into a dataframe\n",
    "\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect.get_feature_names()\n",
    "X_counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bittensorflowconda0aaac7ec2512440c880b9d55ddeeefc5",
   "display_name": "Python 3.6.10 64-bit ('tensorflow': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}